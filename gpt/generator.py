# ğŸ“ gpt/generator.py (OpenRouter ìš°íšŒ ë²„ì „)
import openai
import os
from dotenv import load_dotenv
from tools.paths import (
    PROMPT_PATHS,            # ğŸ“‚ ë„êµ¬ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê²½ë¡œ
    NEW_QUESTIONS_PATH,      # ğŸ“„ ìƒì„±ëœ ë¬¸ì œ ì €ì¥ ìœ„ì¹˜
    ENV_PATH                 # ğŸ” í™˜ê²½ ë³€ìˆ˜ (.env) íŒŒì¼ ê²½ë¡œ
)

# ğŸ” .env íŒŒì¼ì—ì„œ API í‚¤ ë° OpenRouter ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv(dotenv_path=ENV_PATH)
openai.api_key = os.getenv("OPENROUTER_API_KEY")
openai.api_base = "https://openrouter.ai/api/v1"  # âœ… OpenRouter API URL

# ğŸ“¥ í…œí”Œë¦¿ íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
def load_prompt_template(tool):
    """ë„êµ¬ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    path = PROMPT_PATHS[tool]
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

# ğŸš€ ë¬¸ì œ ìƒì„± ìš”ì²­ í•¨ìˆ˜ (OpenRouter LLM ì‚¬ìš©)
def generate_questions(dataset, tool, model="mistralai/mistral-7b-instruct"):
    """LLMì—ê²Œ ë¬¸ì œë¥¼ ìš”ì²­í•˜ê³  ì‘ë‹µì„ ë°›ì•„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    prompt = load_prompt_template(tool).format(dataset=dataset)

    response = openai.ChatCompletion.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ë°ì´í„° ë¶„ì„ ë¬¸ì œë¥¼ ë§Œë“œëŠ” AIì•¼."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=1000
    )

    return response.choices[0].message.content

# ğŸ’¾ ìƒì„±ëœ ë¬¸ì œ ì €ì¥ í•¨ìˆ˜
def save_questions(text):
    with open(NEW_QUESTIONS_PATH, "a", encoding="utf-8") as f:
        f.write(text.strip() + "\n")

# â–¶ï¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    dataset = "titanic"
    tool = "pds"
    result = generate_questions(dataset, tool)
    print("\nğŸ“˜ ìƒì„±ëœ ë¬¸ì œ:")
    print(result)
    save_questions(result)
