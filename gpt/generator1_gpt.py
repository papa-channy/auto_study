# ğŸ“ gpt/generator.py (openai >= 1.0.0 ëŒ€ì‘ ë²„ì „)
import openai
import os
from dotenv import load_dotenv
from tools.paths import (
    PROMPT_PATHS,            # ğŸ“‚ ë„êµ¬ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê²½ë¡œ
    NEW_QUESTIONS_PATH,      # ğŸ“„ ìƒì„±ëœ ë¬¸ì œ ì €ì¥ ìœ„ì¹˜
    ENV_PATH                 # ğŸ” í™˜ê²½ ë³€ìˆ˜ (.env) íŒŒì¼ ê²½ë¡œ
)

# ğŸ” .env íŒŒì¼ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv(dotenv_path=ENV_PATH)
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ğŸ“¥ í…œí”Œë¦¿ íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
def load_prompt_template(tool):
    """ë„êµ¬ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    path = PROMPT_PATHS[tool]
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

# ğŸš€ GPTì—ê²Œ ë¬¸ì œ ìƒì„± ìš”ì²­ í•¨ìˆ˜ (openai>=1.0.0 ë²„ì „)
def generate_questions(dataset, tool, model="gpt-3.5-turbo"):
    """GPTì—ê²Œ ë¬¸ì œë¥¼ ìš”ì²­í•˜ê³  ì‘ë‹µì„ ë°›ì•„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    prompt = load_prompt_template(tool).format(dataset=dataset)

    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ë¬¸ì œë¥¼ ë§Œë“œëŠ” AIì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=1000
    )

    return response.choices[0].message.content  # ğŸ§  ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ

# ğŸ’¾ ìƒì„±ëœ ë¬¸ì œë¥¼ íŒŒì¼ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_questions(text):
    with open(NEW_QUESTIONS_PATH, "a", encoding="utf-8") as f:
        f.write(text.strip() + "\n")

# ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìš©: ë¬¸ì œ ìƒì„± í›„ ì €ì¥ê¹Œì§€
if __name__ == "__main__":
    dataset = "titanic"
    tool = "pds"
    result = generate_questions(dataset, tool)
    print("\nğŸ“˜ ìƒì„±ëœ ë¬¸ì œ:")
    print(result)
    save_questions(result)
